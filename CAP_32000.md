  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

### MÉTODO DO ENXAME DE PARTÍCULAS

<p align="justify">
<font size="2" face="CMU Serif">  
O método de Enxame de Partículas (em inglês <i>Particle Swarm Optimizatio</i>-PSO) foi proposto por James Kennedy e Russell Elberthart em 1995 [<font color="red">EULLER AQUI VOCÊ TEM QUE JÁ CITAR O ARTIGO DO PSO O ORIGINAL COLOQUE COMO URL O LINK DO ARTIGO VOU TE DAR UM EXEMPLO VEJA A FRENTE</font>] <a href="https://arxiv.org/ftp/arxiv/papers/1804/1804.05319.pdf">[1]</a>. O Enxame de Partículas ou <b>PSO</b> é um algoritmo metaheurístico baseado em inteligência de enxames que busca simular o comportamento de pássaros e cardume de peixes em busca de alimentos, cada um desses animais são chamados de partícula no método computacional. A particula possue duas propriedades principais que são: (a) a sua posição (\(x_{i}\)); e (b) sua velocidade (\(v_{i}\)), ambas representadas vetorialmente em um espaço \(n\)-dimensional [Iraj Koohi e Voicu Z. Groza <font color="red">EULLER AQUI TROCAR O NOME INTEIRO DOS CARAS POR NÚMERO....ENTÃO NOSSAS REFERÊNCIAS DEVEM SER TODAS NUMÉRICAS 1 2 3 4 5 E ASSIM POR DIANTE COMO FOI NO RELATÓRIO DE IC</font>]. Além disso o método possui uma espécie de memória que armazena um histórico das melhores posições (em relação à aptidão) no espaço \(n\)-dimensional. O histórico de armazenamento relativo a cada partícula é denominado de \(p_{best}\) (em inglês <i>personal best</i>) e o melhor individuo do enxame também terá sua posição armazenada sendo chamado de \(g_{best}\) (em inglês <i>global best</i>). A posição (\(x_{i}\)) e a velocidade (\ (\(v_{i}\)) são modificadas com base nas equações (1) e (2).
</font>  
</p>

<p>
  When \(a \ne 0\), there are two solutions to \(ax^2 + bx + c = 0\) and they are
  \[x = {-b \pm \sqrt{b^2-4ac} \over 2a}.\]
</p>

$$(1) v_{ji}(t+1)=wv_{ji}+c_1r_1(pB_{ij}(t)-x_{ij}(t))+c_2r_2(gB_{ij}(t)-x_{ij}(t)) $$

$$(2) x_{ij}(t+1)=x_{ij}(t)+v_{ij} $$

Onde:
- $$ v_{ji}(t+1) $$-é a velocidade da partícula i na iteração j. 
- $$ x_{ij}(t+1) $$-é a posição da partícula i na iteração j.
- $$ w $$-é o coeficiente de inércia.
- $$ c_1 $$-é o fator de aprendizado cognitivo.
- $$ c_2 $$-é o fator social de aprendizagem.
- $$r1 e r2$$-são números aleatórios em entre 0 e 1. 
  

  Os valores de $$ v $$ podem ser fixados em uma faixa quem são $$[vmin, vmax]$$  para que a partícula são saía do espaço de busca. O algoritmo PSO termina com gerações máximas ou a melhor posição de partícula do enxame, que não pode ser melhorado depois de uma quantidade suficientemente grande número de gerações.O funcionamento do algoritmo PSO está logo abaixo:

``` python
    Inicio Algoritmo
        Para i - 0 até n faça
            Para j de 1 a d faça
                x:-rand(min, max)
                v:-rand(0, vmax)
            Fim Para
            p:-x
            pbest - f(pi)
            Se pbest < vbest então
                gbest:= pbest
            Fim Se
        Fim Para
        Para i= 0 até n faça
            Para j = 0 até m faça
                v := w*v + c1*rand()*(pbest-x) + c2*rand()*(gbest-x)
                x:= v + x
            Fim Para
            fitness := f(x)
            Se fitness < pbest então
                pbest := fitness
                Se fitness < vbest então
                    gbest[i]:= fitness
                Fim Se
            Fim Se
        Fim Para
    Fim Algoritmo
```

